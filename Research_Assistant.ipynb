{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdfahad999/Experiments/blob/main/Research_Assistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWjaYn2uPkXW",
        "outputId": "98216e84-7c41-43c5-deb4-d305c11bbf72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.338-py3-none-any.whl (2.0 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m1.8/2.0 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openai\n",
            "  Downloading openai-1.3.3-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting duckduckgo-search\n",
            "  Downloading duckduckgo_search-3.9.6-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith\n",
            "  Downloading langsmith-0.0.65-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.5.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.8.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: aiofiles>=23.2.1 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (23.2.1)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (8.1.7)\n",
            "Requirement already satisfied: lxml>=4.9.3 in /usr/local/lib/python3.10/dist-packages (from duckduckgo-search) (4.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: httpcore in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Collecting socksio==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading socksio-1.0.0-py3-none-any.whl (12 kB)\n",
            "Collecting brotli (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading Brotli-1.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m92.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2<5,>=3 (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.14.3)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3->httpx<1,>=0.23.0->openai)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3->httpx<1,>=0.23.0->openai)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Installing collected packages: brotli, socksio, mypy-extensions, marshmallow, jsonpointer, hyperframe, hpack, typing-inspect, tiktoken, jsonpatch, h2, openai, langsmith, dataclasses-json, langchain, duckduckgo-search\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed brotli-1.1.0 dataclasses-json-0.6.2 duckduckgo-search-3.9.6 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.338 langsmith-0.0.65 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-1.3.3 socksio-1.0.0 tiktoken-0.5.1 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain  openai  tiktoken  duckduckgo-search langsmith requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
        "from langchain.utilities import DuckDuckGoSearchAPIWrapper\n",
        "import json\n",
        "\n",
        "openai_key=''\n",
        "\n",
        "RESULTS_PER_QUESTION = 3\n",
        "ddg_search = DuckDuckGoSearchAPIWrapper()\n",
        "\n",
        "\n",
        "def web_search(query: str, num_results: int = RESULTS_PER_QUESTION):\n",
        "    results = ddg_search.results(query, num_results)\n",
        "    return [r[\"link\"] for r in results]\n",
        "\n",
        "\n",
        "# SUMMARY_TEMPLATE = \"\"\"Summarize the following question based on the given content:\n",
        "# Question :{question}\n",
        "\n",
        "# Content:{content}\"\"\"  # noqa: E501\n",
        "\n",
        "\n",
        "# SUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n",
        "\n",
        "SUMMARY_TEMPLATE = \"\"\"{text}\n",
        "-----------\n",
        "Using the above text, answer in short the following question:\n",
        "> {question}\n",
        "-----------\n",
        "if the question cannot be answered using the text, imply summarize the text. Include all factual information, numbers, stats etc if available.\"\"\"  # noqa: E501\n",
        "SUMMARY_PROMPT = ChatPromptTemplate.from_template(SUMMARY_TEMPLATE)\n",
        "\n",
        "\n",
        "def scrape_text(url: str):\n",
        "    print(f'URL entered:{url}')\n",
        "    # Send a GET request to the webpage\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Parse the content of the request with BeautifulSoup\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "            # Extract all text from the webpage\n",
        "            page_text = soup.get_text(separator=\" \", strip=True)\n",
        "\n",
        "            # Print the extracted text\n",
        "\n",
        "            return page_text\n",
        "        else:\n",
        "            return f\"Failed to retrieve the webpage: Status code {response.status_code}\"\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return f\"Failed to retrieve the webpage: {e}\"\n",
        "\n",
        "\n",
        "url = \"https://blog.langchain.dev/announcing-langsmith/\"\n",
        "\n",
        "#url='https://www.exlservice.com/insights/article/risk-convergence-how-insurers-can-seize-initiative-privacy-cyber-risk-and'\n",
        "\n",
        "\n",
        "page_content =scrape_text(url)[:10000]\n",
        "\n",
        "print(page_content[:])\n",
        "\n",
        "chain = SUMMARY_PROMPT | ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=openai_key) | StrOutputParser()\n",
        "\n",
        "#question='how insurers can seize initiative privacy cyber-risk'\n",
        "\n",
        "question='what is the langsmith all about'\n",
        "\n",
        "\n",
        "scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
        "    summary = RunnablePassthrough.assign(\n",
        "    text=lambda x: scrape_text(x[\"url\"])[:10000]\n",
        ") | SUMMARY_PROMPT | ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=openai_key) | StrOutputParser()\n",
        ") | (lambda x: f\"\\n\\n SUMMARY: {x['summary']}\\n\\n REFERENCE URL: {x['url']}\")\n",
        "\n",
        "\n",
        "\n",
        "web_search_chain = RunnablePassthrough.assign(\n",
        "    urls = lambda x: web_search(x[\"question\"])\n",
        ") | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()\n",
        "\n",
        "\n",
        "##Adding prompt for generating 3 question about the original query and then curreating all in the information in one report in markdown format\n",
        "\n",
        "\n",
        "SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Write 3 google search queries to search online that form an \"\n",
        "            \"objective opinion from the following: {question}\\n\"\n",
        "            \"You must respond with a list of strings in the following format: \"\n",
        "            '[\"query 1\", \"query 2\", \"query 3\"].',\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "search_question_chain = SEARCH_PROMPT | ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=openai_key, temperature=0) | StrOutputParser() | json.loads\n",
        "\n",
        "full_research_chain = search_question_chain | (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()\n",
        "\n",
        "WRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"  # noqa: E501\n",
        "\n",
        "\n",
        "# Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
        "RESEARCH_REPORT_TEMPLATE = \"\"\"Information:\n",
        "--------\n",
        "{research_summary}\n",
        "--------\n",
        "Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
        "The report should focus on the answer to the question, should be well structured, informative, \\\n",
        "in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
        "You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
        "You must write the report with markdown syntax.\n",
        "You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
        "Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
        "You must write the report in apa format.\n",
        "Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", WRITER_SYSTEM_PROMPT),\n",
        "        (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
        "    ]\n",
        ")\n",
        "\n",
        "def collapse_list_of_lists(list_of_lists):\n",
        "    content = []\n",
        "    for l in list_of_lists:\n",
        "        content.append(\"\\n\\n\".join(l))\n",
        "    return \"\\n\\n\".join(content)\n",
        "\n",
        "chain = RunnablePassthrough.assign(\n",
        "    research_summary= full_research_chain | collapse_list_of_lists\n",
        ") | prompt | ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=openai_key) | StrOutputParser()\n",
        "\n",
        "\n",
        "\n",
        "# result=chain.invoke(\n",
        "#     {'question':question,\n",
        "#      }\n",
        "#     )\n",
        "\n",
        "# result=web_search_chain.invoke(\n",
        "#     {'question':question,\n",
        "#      }\n",
        "#     )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egwfsdUdi4hT",
        "outputId": "852b1e20-77d6-4555-e9e4-070b56e62fd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL entered:https://blog.langchain.dev/announcing-langsmith/\n",
            "Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications Skip to content LangChain Blog Home By LangChain Release Notes GitHub Docs Case Studies Sign in Subscribe Announcing LangSmith, a unified platform for debugging, testing, evaluating, and monitoring your LLM applications 8 min read Jul 18, 2023 LangChain exists to make it as easy as possible to develop LLM-powered applications. We started with an open-source Python package when the main blocker for building LLM-powered applications was getting a simple prototype working. We remember seeing Nat Friedman tweet in late 2022 that there was “ not enough tinkering happening .” The LangChain open-source packages are aimed at addressing this and we see lots of tinkering happening now ( Nat agrees )–people are building everything from chatbots over internal company documents to an AI dungeon master for a Dungeons and Dragons game. The blocker has now changed. While it’s easy to build a prototype of an application in ~5 lines of LangChain code, it’s still deceptively hard to take an application from prototype to production. The main issue that we see today is application performance–something that works ~30% of the time is good enough for a Twitter demo, but not nearly good enough for production. Today, we’re introducing LangSmith, a platform to help developers close the gap between prototype and production. It’s designed for building and iterating on products that can harness the power–and wrangle the complexity–of LLMs. LangSmith is now in closed beta. So if you’re looking for a robust, unified, system for debugging, testing, evaluating, and monitoring your LLM applications, sign up here . How did we get here? Given the stochastic nature of LLMs, it is not easy–and there’s currently no straightforward way–to answer the simple question of “what’s happening in these models?,” let alone getting them to work reliably. The builders we hear from are running into the same roadblocks (and it’s true for our team, too): Understanding what exactly the final prompt to the LLM call is (after all the prompt template formatting, this final prompt can be long and obfuscated) Understanding what exactly is returned from the LLM call at each step (before it is post-processed or transformed in any way) Understanding the exact sequence of calls to LLM (or other resources), and how they are chained together Tracking token usage Managing costs Tracking (and debugging) latency Not having a good dataset to evaluate their application over Not having good metrics with which to evaluate their application Understanding how users are interacting with the product All of these problems have parallels in traditional software engineering. And, in response, a set of practices and tools for debugging, testing, logging, monitoring, etc. has emerged to help developers abstract away common infrastructure and focus on what really matters - building their applications. LLM application developers deserve the same. LangSmith aspires to be that platform. Over the last few months, we’ve been working directly with some early design partners and testing it on our own internal workflows, and we’ve found LangSmith helps teams in 5 core ways: Debugging LangSmith gives you full visibility into model inputs and output of every step in the chain of events. This makes it easy for teams to experiment with new chains and prompt templates, and spot the source of unexpected results, errors, or latency issues. We’ll also expose latency and token usage so that you can identify which calls are causing issues. We’ve also made it easy to change and rerun examples from the UI. We added this feature after seeing teams take logs of bad examples and copy-paste into the OpenAI playground to tweak the prompt until they got a good result. We wanted to eliminate that friction, and now with the click of a button, you can go from a log to a playground where you can actively edit. This is currently supported for both OpenAI and Anthropic models, with support for more coming soon. We’re also working on supporting this for chains in general. This deep visibility into model performance has been particularly helpful for teams developing complex applications. LangSmith helped Streamlit and Snowflake implement agents that could intelligently and reliably answer questions about their data. \"LangChain has been instrumental in helping us prototype intelligent agents at Snowflake,” said Adrien Treuille, Director of Product at Snowflake. “LangSmith was easy to integrate, and the agnostic open source API made it very flexible to adapt to our implementation,” tacked on Richard Meng, Senior Software Engineer at Snowflake. Boston Consulting Group also built a highly-customized, and highly performant, series of applications on top of LangChain’s framework by relying on this same infrastructure. “We are proud of being one of the early LangChain design partners and users of LangSmith,”  Said Dan Sack, Managing Director and Partner at BCG.  “The use of LangSmith has been key to bringing production-ready LLM applications to our clients.  LangSmith's ease of integration and intuitive UI enabled us to have an evaluation pipeline up and running very quickly. Additionally, tracing and evaluating the complex agent prompt chains is much easier, reducing the time required to debug and refine our prompts, and giving us the confidence to move to deployment.” In another example of debugging in action, we partnered with DeepLearningAI to equip learners in the recently-released LangChain courses with access to LangSmith. This allowed students to easily visualize the exact sequence of calls, and the inputs and outputs at each step in the chain with precision. Students can understand exactly what the chains, prompts, and LLMs were doing, which helps build intuition as they learn to create new and more sophisticated applications. Testing One of the main questions we see developers grapple with is: “If I change this chain/prompt, how does that affect my outputs?” The most effective way to answer this question is to curate a dataset of examples that you care about, and then run any changed prompts/chains over this dataset. LangSmith first makes it easy to create these datasets from traces or by uploading datasets you’ve curated manually. You can then easily run chains and prompts over those data sets. The first helpful step is simply manually looking at the new inputs and outputs. Although this may seem unsatisfyingly basic, it actually has some benefits - many of the companies we’ve spoken to actually like some manual touch points because it allows them to develop better intuition about how to interact with LLMs. This intuition can prove incredibly valuable when trying to think about how to improve the application. The main unlock we hope to provide is a clear interface for letting developers easily see the inputs and outputs for each data point, as without that visibility they cannot build up that intuition. Today, we primarily hear from teams that want to bring their prototype into production, and are narrowing in on specific prompts they’d like to improve. Klarna is building industry-leading AI integrations that go beyond a simple call to a language model, and instead rely on a series of calls. As they focus on a specific section, LangSmith has provided the tools and data they need to ensure no regressions occur. In parallel, we’re starting to hear from more and more ambitious teams that are striving for a more effective approach. Evaluating LangSmith integrates seamlessly with our open source collection of evaluation modules. These modules have two main types of evaluation: heuristics and LLMs. Heuristic Evaluations will use logic like regexes to evaluate the correctness of an answer. LLM evaluations will use LLMs to evaluate themselves. We are extremely bullish on LLM assisted evaluation over the long term. Critics of this approach will say that it’s conceptually shaky and practically costly (time and money). But, we’ve been seeing some very compelling evidence come out of top labs that this is a viable strategy. And, as we collectively make improvements to these models–both private and open source–and usage becomes more ubiquitous, we expect costs to come down considerably. Monitoring While debugging, testing, and evaluating can help you get from prototype to production, the work doesn’t stop once you ship. Developers need to actively track performance, and ideally, optimize that performance based on feedback. We consistently see developers relying on LangSmith to track the system-level performance of their application (like latency and cost), track the model/chain performance (through associating feedback with runs), debug issues (diving into a particular run that went wrong), and establish a broad understanding of how users are interacting with their application and what their experience is like. Ambitious startups like Mendable , Multi-On and Quivr , who are already serving thousands of users are actively using LangSmith to not only monitor overall usage, but also use those insights to take action on critical issues. “Thanks to Langchain smith we were able to analyze our LLM calls, understand the performance of the different chain methods ( stuff vs reduce) for QA and improve upon it. It even helped us debug and understand errors we made. We are consistently using it to improve our prompt engineering and look forward to the new features,” said Stan Girard, Head of GenAI at Theodo and creator of Quivr . A unified platform While each of these product areas provide unique value, often at a specific point in time in the development process, we believe a great deal of the long term impact of LangSmith will come from having a single, fully-integrated hub to do this work from. We see teams with all kinds of Rube Goldberg-machine-like processes for managing their LLM applications, and we want to make that a thing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(result )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL8Ojxi1mCDz",
        "outputId": "cd51198b-5067-484f-8538-a5dde0924ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"\\n\\n SUMMARY: LangSmith is a new product by the creators of LangChain, aimed at helping developers get their large language models (LLM’s) into production in a reliable and maintainable way. It focuses on solving production challenges around reliability, debugging, testing, evaluating, monitoring, and usage metrics. The goal is to provide a simple and intuitive UI to reduce the barrier to entry for developers without a software background. LangSmith aims to be differentiated by offering extensibility and integrations with other tools, while also connecting with as many tools as possible. Competitors in this space include Vercel, AI SDK providers, and Embeddings providers. LangSmith's long-term success will depend on its ability to continue expanding in scope and remaining competitive with multiple providers and other tooling ecosystems.\\n\\n REFERENCE URL: https://logankilpatrick.medium.com/what-is-langsmith-and-why-should-i-care-as-a-developer-e5921deb54b5\", '\\n\\n SUMMARY: LangSmith is a tool developed to address the production challenges of large language models (LLMs) and ensure applications can be deployed in a reliable and maintainable way. It focuses on providing features for debugging, testing, evaluating, monitoring, and usage metrics, all accessible through a simple and intuitive UI. The goal is to reduce the barrier to entry for developers without a software background and provide visualizations to aid in understanding the LLM system. LangSmith also aims to connect with other tools and platforms, and is positioned to compete with similar tooling in the LLM ecosystem.\\n\\n REFERENCE URL: https://dev.to/logankilpatrick/what-is-langsmith-and-why-should-i-care-as-a-developer-19k', '\\n\\n SUMMARY: LangSmith is an innovative and dynamic testing framework for evaluating language models and AI applications. It is capable of building production-grade LLM applications and provides tools that analyze and extract valuable insights from model responses. LangSmith is particularly useful for refining and enhancing AI applications, optimizing them for real-world usage scenarios. It offers features such as quick debugging, customizable test scenarios, metrics and analytics, and interactive visualization to measure the strengths and weaknesses of language models. LangSmith also builds on top of LangChain, which is used for creating prototypes and offers tracing tools for investigating and debugging an agent’s execution steps. The platform aims to provide developers with the ability to thoroughly test and evaluate language models, ensuring that they meet user expectations and perform reliably.\\n\\n REFERENCE URL: https://blog.logrocket.com/langsmith-test-llms-ai-applications/']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Given string representation of a list\n",
        "string_representation = '[\"Langchain vs Langsmith differences\"]'\n",
        "\n",
        "# Convert the string to a Python list\n",
        "python_list = json.loads(string_representation.replace(\"'\", '\"'))\n",
        "\n",
        "# Print the Python list\n",
        "print(\"Python List:\", python_list)\n",
        "\n",
        "# Convert the Python list to JSON\n",
        "json_representation = json.dumps(python_list)\n",
        "\n",
        "# Print the JSON representation\n",
        "print(\"JSON Representation:\", json_representation)"
      ],
      "metadata": {
        "id": "cVXi-71y_i2J",
        "outputId": "d2739a66-6f58-416b-e02d-c1ba4ee054f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python List: ['Langchain vs Langsmith differences']\n",
            "JSON Representation: [\"Langchain vs Langsmith differences\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install gradio"
      ],
      "metadata": {
        "id": "gmyXT2LrtstQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def Research_assistant(question):\n",
        "\n",
        "\n",
        "    scrape_and_summarize_chain = RunnablePassthrough.assign(\n",
        "        summary = RunnablePassthrough.assign(\n",
        "        text=lambda x: scrape_text(x[\"url\"])[:10000]\n",
        "    ) | SUMMARY_PROMPT | ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=openai_key) | StrOutputParser()\n",
        "    ) | (lambda x: f\"\\n\\n SUMMARY: {x['summary']}\\n\\n REFERENCE URL: {x['url']}\")\n",
        "\n",
        "\n",
        "\n",
        "    web_search_chain = RunnablePassthrough.assign(\n",
        "        urls = lambda x: web_search(x[\"question\"])\n",
        "    ) | (lambda x: [{\"question\": x[\"question\"], \"url\": u} for u in x[\"urls\"]]) | scrape_and_summarize_chain.map()\n",
        "\n",
        "\n",
        "\n",
        "    # result=web_search_chain.invoke(\n",
        "    #     {'question':question,\n",
        "    #     }\n",
        "    #     )\n",
        "    #print('Type of the result', type(result))\n",
        "\n",
        "    ##Adding prompt for generating 3 question about the original query and then curreating all in the information in one report in markdown format\n",
        "\n",
        "    SEARCH_PROMPT = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\n",
        "                \"user\",\n",
        "                \"Write 3 google search queries to search online that form an \"\n",
        "                \"objective opinion from the following: {question}\\n\"\n",
        "                \"You must strictly  respond with a list and convert to strings in the following format: \"\n",
        "                \"\"\"```[\"query 1\", \"query 2\", \"query 3\"]\n",
        "                    ```\n",
        "                    Everything between the ``` must be valid array.\"\"\",\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    def make_questions(questions):\n",
        "\n",
        "\n",
        "        print(f'Questions:: {questions}')\n",
        "        # result_list = []\n",
        "        # result_list.extend(questions)\n",
        "        data=json.loads(str(questions))\n",
        "        print('Done loading')\n",
        "        return data\n",
        "\n",
        "\n",
        "    search_question_chain = SEARCH_PROMPT |ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=openai_key) | StrOutputParser() | make_questions\n",
        "\n",
        "    # def make_questions(questions):\n",
        "    #     print(f'Questions::'  {questions})\n",
        "    #     new=[]\n",
        "    #     for q in questions:\n",
        "    #         new.append({\"question\":q})\n",
        "    #     return new\n",
        "\n",
        "    full_research_chain = search_question_chain | (lambda x: [{\"question\": q} for q in x]) | web_search_chain.map()\n",
        "    #full_research_chain = search_question_chain | make_questions | web_search_chain.map()\n",
        "\n",
        "    WRITER_SYSTEM_PROMPT = \"You are an AI critical thinker research assistant. Your sole purpose is to write well written, critically acclaimed, objective and structured reports on given text.\"  # noqa: E501\n",
        "\n",
        "\n",
        "    # Report prompts from https://github.com/assafelovic/gpt-researcher/blob/master/gpt_researcher/master/prompts.py\n",
        "    RESEARCH_REPORT_TEMPLATE = \"\"\"Information:\n",
        "    --------\n",
        "    {research_summary}\n",
        "    --------\n",
        "    Using the above information, answer the following question or topic: \"{question}\" in a detailed report -- \\\n",
        "    The report should focus on the answer to the question, should be well structured, informative, \\\n",
        "    in depth, with facts and numbers if available and a minimum of 1,200 words.\n",
        "    You should strive to write the report as long as you can using all relevant and necessary information provided.\n",
        "    You must write the report with markdown syntax.\n",
        "    You MUST determine your own concrete and valid opinion based on the given information. Do NOT deter to general and meaningless conclusions.\n",
        "    Write all used source urls at the end of the report, and make sure to not add duplicated sources, but only one reference for each.\n",
        "    You must write the report in apa format.\n",
        "    Please do your best, this is very important to my career.\"\"\"  # noqa: E501\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", WRITER_SYSTEM_PROMPT),\n",
        "            (\"user\", RESEARCH_REPORT_TEMPLATE),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    def collapse_list_of_lists(list_of_lists):\n",
        "        content = []\n",
        "        for l in list_of_lists:\n",
        "            content.append(\"\\n\\n\".join(l))\n",
        "        return \"\\n\\n\".join(content)\n",
        "\n",
        "    chain = RunnablePassthrough.assign(\n",
        "        research_summary= full_research_chain | collapse_list_of_lists\n",
        "    ) | prompt | ChatOpenAI(model=\"gpt-3.5-turbo\",api_key=openai_key) | StrOutputParser()\n",
        "    print('RESEARCH_REPORT_TEMPLATE completed')\n",
        "\n",
        "\n",
        "    result=chain.invoke(\n",
        "        {'question':question,\n",
        "        }\n",
        "        )\n",
        "    print('Type of the result', type(result))\n",
        "\n",
        "    if isinstance(result, list):\n",
        "\n",
        "        return ''.join(result)\n",
        "    else:\n",
        "\n",
        "        return result\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=Research_assistant,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Enter your text here bro!!!\"),\n",
        "    outputs=\"text\",\n",
        ")\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "AI15y8Ye-MRq",
        "outputId": "3c723a05-b01c-4651-a0c4-7c017f51d73c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://8fe4333691ea202534.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8fe4333691ea202534.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESEARCH_REPORT_TEMPLATE completed\n",
            "Questions:: [\"Pros and cons of Meta Glasses\", \"Advantages and disadvantages of Meta Glasses\", \"Meta Glasses: benefits and drawbacks\"]\n",
            "Done loading\n",
            "URL entered:https://www.wired.com/review/review-ray-ban-meta-smart-glasses/\n",
            "URL entered:https://www.wired.com/review/review-ray-ban-meta-smart-glasses/\n",
            "URL entered:https://www.tomsguide.com/reviews/ray-ban-meta-smart-glasses\n",
            "URL entered:https://www.howtogeek.com/ray-ban-meta-smart-glasses-review/URL entered:https://www.wired.com/review/review-ray-ban-meta-smart-glasses/\n",
            "\n",
            "URL entered:https://www.tomsguide.com/reviews/ray-ban-meta-smart-glasses\n",
            "URL entered:https://www.tomsguide.com/reviews/ray-ban-meta-smart-glasses\n",
            "URL entered:https://www.theverge.com/23922425/ray-ban-meta-smart-glasses-review\n",
            "URL entered:https://www.theverge.com/23922425/ray-ban-meta-smart-glasses-review\n",
            "('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "Type of the result <class 'str'>\n",
            "RESEARCH_REPORT_TEMPLATE completed\n",
            "Questions:: [\"difference between iPhone and Samsung S23 Ultra\", \"comparison of features iPhone and Samsung S23 Ultra\", \"specifications and performance comparison iPhone and Samsung S23 Ultra\"]\n",
            "Done loading\n",
            "URL entered:https://www.tomsguide.com/news/galaxy-s23-ultra-vs-iphone-14-pro-max\n",
            "URL entered:https://www.tomsguide.com/face-off/iphone-15-pro-max-vs-samsung-galaxy-s23-ultra\n",
            "URL entered:https://www.cnn.com/cnn-underscored/electronics/iphone-15-pro-vs-galaxy-s23-ultra\n",
            "URL entered:https://www.tomsguide.com/face-off/iphone-15-pro-max-vs-samsung-galaxy-s23-ultra\n",
            "URL entered:https://www.pcmag.com/news/samsung-galaxy-s23-vs-s23-plus-vs-s23-ultra-whats-the-difference\n",
            "URL entered:https://www.tomsguide.com/face-off/iphone-15-pro-max-vs-samsung-galaxy-s23-ultra\n",
            "URL entered:https://www.techradar.com/phones/iphone-15-pro-max-vs-samsung-galaxy-s23-ultraURL entered:https://www.cnn.com/cnn-underscored/electronics/iphone-15-pro-vs-galaxy-s23-ultra\n",
            "\n",
            "URL entered:https://www.pcmag.com/news/apple-iphone-15-vs-samsung-galaxy-s23-which-flagship-is-best\n",
            "Type of the result <class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DQZmZd9HZuQ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}